{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11012548,"sourceType":"datasetVersion","datasetId":6856523},{"sourceId":11041215,"sourceType":"datasetVersion","datasetId":6877567},{"sourceId":11042193,"sourceType":"datasetVersion","datasetId":6878300},{"sourceId":11128124,"sourceType":"datasetVersion","datasetId":6940100},{"sourceId":11204451,"sourceType":"datasetVersion","datasetId":6995855},{"sourceId":228711590,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CQ-Gens Task\nThis notebook demonstrates a system to generate critical questions (CQs) for interventions using advanced NLP techniques. The workflow includes:\n- Loading and normalizing a dataset.\n- Utilizing LLaMA and SentenceTransformer models for question generation and ranking.\n- Adaptive generation with meta-evaluation.\n- External evaluation of predictions.\n\nLet's start by setting up the environment and loading necessary libraries.","metadata":{}},{"cell_type":"markdown","source":"# Load model: deep-hermes-3-llama-3-8b-preview.Q4_K_M.gguf","metadata":{}},{"cell_type":"code","source":"!pip install llama-cpp-python --no-cache-dir\n!pip install evaluate --no-cache-dir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import Libraries and Setup (Code)","metadata":{}},{"cell_type":"code","source":"import json\nimport subprocess\nimport logging\nimport re\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom llama_cpp import Llama\nfrom sentence_transformers import SentenceTransformer, util\n\n# Download required NLTK data if needed\nnltk.download('punkt', quiet=True)\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Loading\n\nLoad the sample dataset from a JSON file stored in Kaggle's input directory. Error handling ensures robustness if the file is missing or corrupted.","metadata":{}},{"cell_type":"code","source":"# Load dataset\ntry:\n    with open(\"/kaggle/input/dataset-cri/sample.json\", \"r\") as f:\n        sample_data = json.load(f)\n    logging.info(\"Dataset loaded successfully.\")\nexcept Exception as e:\n    logging.error(f\"Failed to load dataset: {e}\")\n    sample_data = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Text Normalization\n\nThis section defines and applies an enhanced punctuation normalization function using NLTK's sentence tokenization. It ensures proper sentence endings and checks punctuation ratio.","metadata":{}},{"cell_type":"code","source":"def enhanced_normalize_text(text, min_punctuation_ratio=0.4):\n    \"\"\"\n    Normalize text by using sentence tokenization to ensure proper punctuation.\n    Adds a period at the end of sentences if missing.\n    \"\"\"\n    if not text:\n        return text\n    sentences = sent_tokenize(text)\n    punctuation_chars = set(\".,;:!?\")\n    norm_sentences = []\n    for sentence in sentences:\n        sentence = sentence.strip()\n        if sentence and sentence[-1] not in punctuation_chars:\n            sentence += \".\"\n        norm_sentences.append(sentence)\n    normalized_text = \" \".join(norm_sentences)\n    count = sum(1 for char in normalized_text if char in punctuation_chars)\n    ratio = count / len(normalized_text) if normalized_text else 0\n    if ratio < min_punctuation_ratio:\n        logging.warning(f\"Low punctuation ratio ({ratio}) detected after normalization.\")\n    return normalized_text\n\n# Apply normalization to interventions\nfor key, data in sample_data.items():\n    sample_data[key][\"intervention\"] = enhanced_normalize_text(data.get(\"intervention\", \"\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Loading\n\nLoad the LLaMA generation model and SentenceTransformer models for similarity, coherence, and relevance scoring. Error handling ensures the pipeline stops if models fail to load.","metadata":{}},{"cell_type":"code","source":"# Load generation model\ngen_model_path = \"/kaggle/input/model-llama/DeepHermes-3-Llama-3-8B-q4.gguf\"\ntry:\n    gen_model = Llama(model_path=gen_model_path, n_ctx=4096)\n    logging.info(\"Generation model loaded successfully.\")\nexcept Exception as e:\n    logging.error(f\"Error loading generation model: {e}\")\n    raise\n\n# Load SentenceTransformer models\ntry:\n    sim_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n    coherence_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n    relevance_model = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v3\")\n    logging.info(\"SentenceTransformer models loaded successfully.\")\nexcept Exception as e:\n    logging.error(f\"Error loading SentenceTransformer models: {e}\")\n    raise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Functions\n\nDefine utility functions for text processing, question extraction, and quality checks to support the generation pipeline.","metadata":{}},{"cell_type":"code","source":"def extract_numbered_questions(text):\n    \"\"\"Extracts numbered sentences ending with a question mark using regex.\"\"\"\n    pattern = re.compile(r\"(?m)^\\s*(\\d+)\\.\\s*(.+\\?)\\s*$\")\n    matches = pattern.findall(text)\n    if matches:\n        return [f\"{num}. {q.strip()}\" for num, q in matches]\n    lines = text.split(\"\\n\")\n    return [line.strip() for line in lines if line.strip().endswith(\"?\")]\n\ndef is_interrogative(sentence):\n    \"\"\"Checks if a sentence is interrogative (ends with a question mark).\"\"\"\n    return sentence.strip().endswith(\"?\")\n\ndef post_process_cqs(cqs):\n    \"\"\"Validate and clean generated critical questions.\"\"\"\n    processed = [cq for cq in cqs if re.match(r\"^\\d+\\.\\s+\", cq) and len(cq.split()) > 5 and is_interrogative(cq)]\n    if not processed:\n        joined = \"\\n\".join(cqs)\n        processed = extract_numbered_questions(joined)\n    return processed if processed else cqs\n\ndef heuristic_quality_check(cqs):\n    \"\"\"Check that questions are diverse (at least two distinct questions).\"\"\"\n    if len(cqs) < 2:\n        return False\n    words_list = [set(cq.lower().split()) for cq in cqs]\n    for i in range(len(words_list)):\n        for j in range(i+1, len(words_list)):\n            overlap = words_list[i].intersection(words_list[j])\n            if len(overlap) / min(len(words_list[i]), len(words_list[j])) > 0.6:\n                return False\n    return True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Question Generation\n\nImplement functions to generate critical questions using few-shot prompting, rank them with semantic models, and perform meta-evaluation.","metadata":{}},{"cell_type":"code","source":"def generate_cqs(text, variation=0, temperature=0.8):\n    \"\"\"Generate exactly three critical questions with few-shot examples.\"\"\"\n    few_shot_example = \"Example:\\n1. How does the argument define its key terms?\\n2. What assumptions are being made without evidence?\\n3. How might alternative perspectives challenge the argument?\\n\\n\"\n    prompts = [\n        f\"Read the following argument:\\n{text}\\n\\n{few_shot_example}Now, generate exactly three critical questions... Start with '1.'\",\n        f\"Consider the argument below:\\n{text}\\n\\n{few_shot_example}Produce exactly three incisive critical questions... Begin with '1.'\"\n    ]\n    prompt = prompts[variation % len(prompts)]\n    try:\n        output = gen_model(prompt, max_tokens=250, temperature=temperature, top_p=0.9, stop=[\"4.\"])\n        generated_text = output[\"choices\"][0][\"text\"].strip()\n        cqs = extract_numbered_questions(generated_text)\n        cqs = post_process_cqs(cqs)\n        logging.info(f\"Generated CQs: {cqs}\")\n        return cqs[:3] if cqs else []\n    except Exception as e:\n        logging.error(f\"Error generating CQs for '{text[:50]}...': {e}\")\n        return []\n\ndef rank_cqs(intervention, cqs):\n    \"\"\"Rank CQs using similarity, coherence, and relevance scores.\"\"\"\n    if not cqs:\n        return []\n    try:\n        intervention_emb_sim = sim_model.encode(intervention, convert_to_tensor=True)\n        cq_embs_sim = sim_model.encode(cqs, convert_to_tensor=True)\n        sim_scores = [util.pytorch_cos_sim(intervention_emb_sim, cq_emb)[0].item() for cq_emb in cq_embs_sim]\n        \n        intervention_emb_coh = coherence_model.encode(intervention, convert_to_tensor=True)\n        cq_embs_coh = coherence_model.encode(cqs, convert_to_tensor=True)\n        coherence_scores = [float(util.pytorch_cos_sim(intervention_emb_coh, coh_emb)[0].item()) for coh_emb in cq_embs_coh]\n\n        intervention_emb_rel = relevance_model.encode(intervention, convert_to_tensor=True)\n        cq_embs_rel = relevance_model.encode(cqs, convert_to_tensor=True)\n        relevance_scores = [float(util.pytorch_cos_sim(intervention_emb_rel, rel_emb)[0].item()) for rel_emb in cq_embs_rel]\n\n        final_scores = [0.4 * sim + 0.3 * coh + 0.3 * rel for sim, coh, rel in zip(sim_scores, coherence_scores, relevance_scores)]\n        ranked = sorted(zip(final_scores, cqs), key=lambda x: x[0], reverse=True)\n        ranked_cqs = [cq for score, cq in ranked]\n        logging.info(f\"Ranked CQs: {ranked_cqs}\")\n        return ranked_cqs[:3]\n    except Exception as e:\n        logging.error(f\"Error ranking CQs for '{intervention[:50]}...': {e}\")\n        return []\n\ndef meta_evaluate_cqs(intervention, cqs):\n    \"\"\"Evaluate CQ effectiveness with a confidence score (1-5).\"\"\"\n    if not cqs:\n        return False\n    eval_prompt = f\"Review the argument: {intervention}\\n\\nCritical Questions:\\n\" + \"\\n\".join([f\"- {cq}\" for cq in cqs]) + \"\\n\\nScore (1-5) how effectively these questions challenge the argument. Provide only a number.\"\n    try:\n        eval_output = gen_model(eval_prompt, max_tokens=50, temperature=0.3, top_p=0.9)\n        eval_response = eval_output[\"choices\"][0][\"text\"].strip()\n        score = float(eval_response) if eval_response.isdigit() else 0\n        logging.info(f\"Meta evaluation score: {score}\")\n        return score >= 3.5 and heuristic_quality_check(cqs)\n    except Exception as e:\n        logging.error(f\"Error during meta evaluation for '{intervention[:50]}...': {e}\")\n        return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Adaptive Generation\n\nImplement an adaptive loop to generate and refine CQs with dynamic parameter adjustments, returning the best set based on meta-evaluation.","metadata":{}},{"cell_type":"code","source":"def adaptive_generation(intervention, max_attempts=5):\n    \"\"\"Adaptive generation with parameter adjustments.\"\"\"\n    attempt = 0\n    temperature = 0.8\n    variation = 0\n    best_cqs = []\n    best_score = 0\n    while attempt < max_attempts:\n        logging.info(f\"Attempt {attempt+1} for '{intervention[:50]}...' (temp={temperature}, variation={variation})\")\n        generated_cqs = generate_cqs(intervention, variation, temperature)\n        if not generated_cqs:\n            temperature = max(0.5, temperature - 0.1)\n            variation += 1\n            attempt += 1\n            continue\n        ranked_cqs = rank_cqs(intervention, generated_cqs)\n        eval_prompt = f\"Review: {intervention}\\n\\nQuestions:\\n\" + \"\\n\".join([f\"- {cq}\" for cq in ranked_cqs]) + \"\\n\\nScore (1-5):\"\n        try:\n            eval_output = gen_model(eval_prompt, max_tokens=50, temperature=0.3, top_p=0.9)\n            current_score = float(eval_output[\"choices\"][0][\"text\"].strip()) if eval_output[\"choices\"][0][\"text\"].strip().isdigit() else 0\n        except Exception as e:\n            current_score = 0\n        if current_score >= 3.5 and heuristic_quality_check(ranked_cqs):\n            return ranked_cqs\n        if current_score > best_score:\n            best_score = current_score\n            best_cqs = ranked_cqs\n        temperature = max(0.5, temperature - 0.1) if temperature > 0.5 else 0.8\n        variation += 1 if temperature <= 0.5 else 0\n        attempt += 1\n    return best_cqs if best_cqs else [\"Could not generate valid questions.\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prediction Generation\n\nProcess all interventions in the dataset to generate and store predictions in a JSON file.","metadata":{}},{"cell_type":"code","source":"def generate_predictions(dataset):\n    \"\"\"Generate predictions for all interventions.\"\"\"\n    predictions = {}\n    for intervention_id, data in dataset.items():\n        intervention_text = data[\"intervention\"]\n        best_cqs = adaptive_generation(intervention_text)\n        if not best_cqs:\n            logging.error(f\"No questions for intervention {intervention_id}\")\n            best_cqs = [\"Could not generate valid questions.\"]\n        predictions[intervention_id] = {\n            \"intervention_id\": intervention_id,\n            \"intervention\": intervention_text,\n            \"dataset\": data.get(\"dataset\", \"unknown\"),\n            \"cqs\": [{\"id\": i, \"cq\": cq} for i, cq in enumerate(best_cqs)]\n        }\n    return predictions\n\n# Execute predictions\nif __name__ == \"__main__\":\n    predictions = generate_predictions(sample_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Predictions and Evaluation\n\nSave the generated predictions to a JSON file and run an external evaluation script to assess quality.","metadata":{}},{"cell_type":"code","source":"# Save predictions\ntry:\n    with open(\"predictions.json\", \"w\") as f:\n        json.dump(predictions, f, indent=2)\n    logging.info(\"Predictions saved successfully in 'predictions.json'.\")\nexcept Exception as e:\n    logging.error(f\"Error saving predictions: {e}\")\n\n# Run external evaluation\ntry:\n    result = subprocess.run([\n        \"python\", \"/kaggle/input/evaluation/evaluation.py\",\n        \"--input_path\", \"/kaggle/input/dataset-cri/sample.json\",\n        \"--submission_path\", \"predictions.json\",\n        \"--metric\", \"similarity\",\n        \"--threshold\", \"0.6\"\n    ], check=True, capture_output=True, text=True)\n    logging.info(\"Evaluation completed successfully.\")\n    print(result.stdout)\nexcept Exception as e:\n    logging.error(f\"Evaluation error: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Open the prediction file","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(\"predictions.json\", \"r\") as f:\n    predictions = json.load(f)\n\n# Print the entire JSON content (for smaller files):\nprint(json.dumps(predictions, indent=4)) #indent=4 makes it easier to read. \n\n# Or, print specific parts of the JSON:\nfor intervention_id, data in predictions.items():\n    print(f\"Intervention ID: {intervention_id}\")\n    print(f\"Intervention: {data['intervention']}\")\n    for cq in data['cqs']:\n        print(f\"  CQ {cq['id']}: {cq['cq']}\")\n    print(\"-\" * 20)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-02T11:16:44.749Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validation phase: Generate predictions on validation dataset using tuned parameters","metadata":{}},{"cell_type":"code","source":"\n# Load dataset\ntry:\n    with open(\"/kaggle/input/dataset-cri/validation.json\", \"r\") as f:\n        sample_data = json.load(f)\n    logging.info(\"Dataset loaded successfully.\")\nexcept Exception as e:\n    logging.error(f\"Failed to load dataset: {e}\")\n    sample_data = {}\n    \nlogging.info(\"Generating predictions on validation dataset using tuned parameters.\")\nvalidation_predictions = generate_predictions(validation_data, tuned_config, use_adaptive=False)\nwith open(\"validation_predictions.json\", \"w\") as f:\n    json.dump(validation_predictions, f, indent=2)\n\ntry:\n    result_val = subprocess.run([\n        \"python\",\n        \"/kaggle/input/evaluation/evaluation.py\",\n        \"--input_path\",\n        \"/kaggle/input/dataset-cri/validation.json\",\n        \"--submission_path\",\n        \"validation_predictions.json\",\n        \"--metric\",\n        \"similarity\",\n        \"--threshold\",\n        str(threshold)\n    ], check=True, capture_output=True, text=True)\n    val_output = result_val.stdout.strip()\n    logging.info(\"Validation evaluation completed successfully.\")\n    logging.info(f\"Validation evaluation output: {val_output}\")\nexcept Exception as e:\n    logging.error(f\"Validation evaluation error: {str(e)}\")\n\n# ---------------------------\n# Final Output: Tuned parameters and evaluation scores\n# ---------------------------\nprint(\"Training complete. Tuned parameters:\")\nprint(tuned_config)\nprint(f\"Train Evaluation Score: {tuned_train_score}\")\nprint(\"Validation predictions generated and evaluated. Check logs for detailed evaluation output.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T07:07:28.867136Z","iopub.execute_input":"2025-04-02T07:07:28.867464Z","iopub.status.idle":"2025-04-02T07:07:29.180329Z","shell.execute_reply.started":"2025-04-02T07:07:28.867439Z","shell.execute_reply":"2025-04-02T07:07:29.179151Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2309739eb85a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating predictions on validation dataset using tuned parameters.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvalidation_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuned_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_adaptive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_predictions.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'validation_data' is not defined"],"ename":"NameError","evalue":"name 'validation_data' is not defined","output_type":"error"}],"execution_count":3}]}